{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook will be building several convolutional neural networks architrctures trying to predict the stock return for several time horizon varying from 5 minutes to many hours.\n",
    "\n",
    "### The idea is to plot the stock price on high frequency (minutes) for a part of the day, for example from market openning until noon, then treat the plot as an image which would be the input to a CNN to predict continuous target  which is the stock return for a remaing horizon in the same day.\n",
    "\n",
    "### This forecast would help us in in buying the stock when it is going up and selling it (short position) when down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data used is related to to S&P500 index and its 500 components and will be loaded as per below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/User/Documents/Data Scientist - Harvard/Advanced Machine Learning/Final Project/sp500/data_stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# will scale each series alone by subtracting the mean only to have small comparable numbers\n",
    "scaler =StandardScaler(with_mean=True,with_std=False)\n",
    "scaler.fit(data.drop(['DATE','SP500'],1))\n",
    "Data_scalled=pd.concat([pd.DataFrame(data['DATE']).assign(SP500=data['SP500']), pd.DataFrame(scaler.transform(data.drop(['DATE','SP500'],1)))], axis=1)\n",
    "Data_scalled.columns=list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41266, 502)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQ.AAL</th>\n",
       "      <th>NASDAQ.AAPL</th>\n",
       "      <th>NASDAQ.ADBE</th>\n",
       "      <th>NASDAQ.ADI</th>\n",
       "      <th>NASDAQ.ADP</th>\n",
       "      <th>NASDAQ.ADSK</th>\n",
       "      <th>NASDAQ.AKAM</th>\n",
       "      <th>NASDAQ.ALXN</th>\n",
       "      <th>...</th>\n",
       "      <th>NYSE.WYN</th>\n",
       "      <th>NYSE.XEC</th>\n",
       "      <th>NYSE.XEL</th>\n",
       "      <th>NYSE.XL</th>\n",
       "      <th>NYSE.XOM</th>\n",
       "      <th>NYSE.XRX</th>\n",
       "      <th>NYSE.XYL</th>\n",
       "      <th>NYSE.YUM</th>\n",
       "      <th>NYSE.ZBH</th>\n",
       "      <th>NYSE.ZTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1491226200</td>\n",
       "      <td>2363.6101</td>\n",
       "      <td>-5.378346</td>\n",
       "      <td>-6.773566</td>\n",
       "      <td>-11.68793</td>\n",
       "      <td>2.593127</td>\n",
       "      <td>-1.250398</td>\n",
       "      <td>-17.778608</td>\n",
       "      <td>8.865648</td>\n",
       "      <td>-1.461163</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.572211</td>\n",
       "      <td>14.294334</td>\n",
       "      <td>-2.264402</td>\n",
       "      <td>-3.163984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.321988</td>\n",
       "      <td>-7.897891</td>\n",
       "      <td>0.576485</td>\n",
       "      <td>-6.833874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1491226260</td>\n",
       "      <td>2364.1001</td>\n",
       "      <td>-5.348346</td>\n",
       "      <td>-6.753566</td>\n",
       "      <td>-10.99793</td>\n",
       "      <td>2.633127</td>\n",
       "      <td>-1.340398</td>\n",
       "      <td>-17.348608</td>\n",
       "      <td>8.945648</td>\n",
       "      <td>-1.501163</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.572211</td>\n",
       "      <td>14.294334</td>\n",
       "      <td>-2.554402</td>\n",
       "      <td>-3.163984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.920718</td>\n",
       "      <td>-4.321988</td>\n",
       "      <td>-8.017891</td>\n",
       "      <td>0.346485</td>\n",
       "      <td>-6.833874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1491226320</td>\n",
       "      <td>2362.6799</td>\n",
       "      <td>-5.398346</td>\n",
       "      <td>-6.763466</td>\n",
       "      <td>-11.09293</td>\n",
       "      <td>2.583127</td>\n",
       "      <td>-1.267898</td>\n",
       "      <td>-17.488608</td>\n",
       "      <td>8.900648</td>\n",
       "      <td>-1.051163</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.357211</td>\n",
       "      <td>14.519334</td>\n",
       "      <td>-2.574402</td>\n",
       "      <td>-3.063984</td>\n",
       "      <td>1.235405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.421988</td>\n",
       "      <td>-8.007891</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>-6.818874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1491226380</td>\n",
       "      <td>2364.3101</td>\n",
       "      <td>-5.338346</td>\n",
       "      <td>-6.813566</td>\n",
       "      <td>-11.24503</td>\n",
       "      <td>2.553127</td>\n",
       "      <td>-1.340398</td>\n",
       "      <td>-17.511408</td>\n",
       "      <td>8.725648</td>\n",
       "      <td>-1.541163</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.482211</td>\n",
       "      <td>14.519334</td>\n",
       "      <td>-2.414402</td>\n",
       "      <td>-3.053984</td>\n",
       "      <td>1.235405</td>\n",
       "      <td>-11.950718</td>\n",
       "      <td>-4.381988</td>\n",
       "      <td>-7.877891</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>-6.803874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1491226440</td>\n",
       "      <td>2364.8501</td>\n",
       "      <td>-5.170546</td>\n",
       "      <td>-6.793566</td>\n",
       "      <td>-11.43793</td>\n",
       "      <td>2.588127</td>\n",
       "      <td>-1.420398</td>\n",
       "      <td>-17.298508</td>\n",
       "      <td>8.725648</td>\n",
       "      <td>-1.381163</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.472211</td>\n",
       "      <td>14.869334</td>\n",
       "      <td>-2.554402</td>\n",
       "      <td>-3.083984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.341988</td>\n",
       "      <td>-7.847891</td>\n",
       "      <td>0.271485</td>\n",
       "      <td>-6.943874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE      SP500  NASDAQ.AAL  NASDAQ.AAPL  NASDAQ.ADBE  NASDAQ.ADI  \\\n",
       "0  1491226200  2363.6101   -5.378346    -6.773566    -11.68793    2.593127   \n",
       "1  1491226260  2364.1001   -5.348346    -6.753566    -10.99793    2.633127   \n",
       "2  1491226320  2362.6799   -5.398346    -6.763466    -11.09293    2.583127   \n",
       "3  1491226380  2364.3101   -5.338346    -6.813566    -11.24503    2.553127   \n",
       "4  1491226440  2364.8501   -5.170546    -6.793566    -11.43793    2.588127   \n",
       "\n",
       "   NASDAQ.ADP  NASDAQ.ADSK  NASDAQ.AKAM  NASDAQ.ALXN    ...      NYSE.WYN  \\\n",
       "0   -1.250398   -17.778608     8.865648    -1.461163    ...    -13.572211   \n",
       "1   -1.340398   -17.348608     8.945648    -1.501163    ...    -13.572211   \n",
       "2   -1.267898   -17.488608     8.900648    -1.051163    ...    -13.357211   \n",
       "3   -1.340398   -17.511408     8.725648    -1.541163    ...    -13.482211   \n",
       "4   -1.420398   -17.298508     8.725648    -1.381163    ...    -13.472211   \n",
       "\n",
       "    NYSE.XEC  NYSE.XEL   NYSE.XL  NYSE.XOM   NYSE.XRX  NYSE.XYL  NYSE.YUM  \\\n",
       "0  14.294334 -2.264402 -3.163984  1.245405 -11.940718 -4.321988 -7.897891   \n",
       "1  14.294334 -2.554402 -3.163984  1.245405 -11.920718 -4.321988 -8.017891   \n",
       "2  14.519334 -2.574402 -3.063984  1.235405 -11.940718 -4.421988 -8.007891   \n",
       "3  14.519334 -2.414402 -3.053984  1.235405 -11.950718 -4.381988 -7.877891   \n",
       "4  14.869334 -2.554402 -3.083984  1.245405 -11.940718 -4.341988 -7.847891   \n",
       "\n",
       "   NYSE.ZBH  NYSE.ZTS  \n",
       "0  0.576485 -6.833874  \n",
       "1  0.346485 -6.833874  \n",
       "2  0.276485 -6.818874  \n",
       "3  0.276485 -6.803874  \n",
       "4  0.271485 -6.943874  \n",
       "\n",
       "[5 rows x 502 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_scalled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXUQkRfx/T9eoy+P0NToAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eaa1c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Data_scalled['NASDAQ.AAL'],data['NASDAQ.AAL'])\n",
    "#plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see above that only the mean is deducted from each series, so it is like we are translating only, this would help to have all series centered around zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the first column contains dates in seconds which differs by 60 seconds since the data is on mimutes basis, we will create a new column called new_day to differentiate days.\n",
    "\n",
    "### In the following code, we will create one chart each day for each stock, so we have 501x106(number of days)=53,106 images. The image will contain the chart during 0.75 of the day and then the log return of that stock for the 0.25 remaining part of the day.\n",
    "\n",
    "### Another return for 30 min ahead will be created\n",
    "\n",
    "### Each day contains 391 so around 293 minutes points (chart) will be used as input to predict the log-return  on the remaining 98 minutes or 30 minutes.\n",
    "\n",
    "### The question is: Can we predict that end of day return based on begining of day behavior? Using CNNs? Are these better than parametric model like ARMA, ARIMA??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_day=np.zeros(shape=(41266,1))\n",
    "new_day[0,0]=1\n",
    "values=data.values\n",
    "for i in range(41265):\n",
    "    if values[i+1,0]==values[i,0]+60:\n",
    "        new_day[i+1,0]=new_day[i,0]\n",
    "    else:\n",
    "        new_day[i+1,0]=new_day[i,0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQ.AAL</th>\n",
       "      <th>NASDAQ.AAPL</th>\n",
       "      <th>NASDAQ.ADBE</th>\n",
       "      <th>NASDAQ.ADI</th>\n",
       "      <th>NASDAQ.ADP</th>\n",
       "      <th>NASDAQ.ADSK</th>\n",
       "      <th>NASDAQ.AKAM</th>\n",
       "      <th>NASDAQ.ALXN</th>\n",
       "      <th>...</th>\n",
       "      <th>NYSE.XEC</th>\n",
       "      <th>NYSE.XEL</th>\n",
       "      <th>NYSE.XL</th>\n",
       "      <th>NYSE.XOM</th>\n",
       "      <th>NYSE.XRX</th>\n",
       "      <th>NYSE.XYL</th>\n",
       "      <th>NYSE.YUM</th>\n",
       "      <th>NYSE.ZBH</th>\n",
       "      <th>NYSE.ZTS</th>\n",
       "      <th>new_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1491226200</td>\n",
       "      <td>2363.6101</td>\n",
       "      <td>-5.378346</td>\n",
       "      <td>-6.773566</td>\n",
       "      <td>-11.68793</td>\n",
       "      <td>2.593127</td>\n",
       "      <td>-1.250398</td>\n",
       "      <td>-17.778608</td>\n",
       "      <td>8.865648</td>\n",
       "      <td>-1.461163</td>\n",
       "      <td>...</td>\n",
       "      <td>14.294334</td>\n",
       "      <td>-2.264402</td>\n",
       "      <td>-3.163984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.321988</td>\n",
       "      <td>-7.897891</td>\n",
       "      <td>0.576485</td>\n",
       "      <td>-6.833874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1491226260</td>\n",
       "      <td>2364.1001</td>\n",
       "      <td>-5.348346</td>\n",
       "      <td>-6.753566</td>\n",
       "      <td>-10.99793</td>\n",
       "      <td>2.633127</td>\n",
       "      <td>-1.340398</td>\n",
       "      <td>-17.348608</td>\n",
       "      <td>8.945648</td>\n",
       "      <td>-1.501163</td>\n",
       "      <td>...</td>\n",
       "      <td>14.294334</td>\n",
       "      <td>-2.554402</td>\n",
       "      <td>-3.163984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.920718</td>\n",
       "      <td>-4.321988</td>\n",
       "      <td>-8.017891</td>\n",
       "      <td>0.346485</td>\n",
       "      <td>-6.833874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1491226320</td>\n",
       "      <td>2362.6799</td>\n",
       "      <td>-5.398346</td>\n",
       "      <td>-6.763466</td>\n",
       "      <td>-11.09293</td>\n",
       "      <td>2.583127</td>\n",
       "      <td>-1.267898</td>\n",
       "      <td>-17.488608</td>\n",
       "      <td>8.900648</td>\n",
       "      <td>-1.051163</td>\n",
       "      <td>...</td>\n",
       "      <td>14.519334</td>\n",
       "      <td>-2.574402</td>\n",
       "      <td>-3.063984</td>\n",
       "      <td>1.235405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.421988</td>\n",
       "      <td>-8.007891</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>-6.818874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1491226380</td>\n",
       "      <td>2364.3101</td>\n",
       "      <td>-5.338346</td>\n",
       "      <td>-6.813566</td>\n",
       "      <td>-11.24503</td>\n",
       "      <td>2.553127</td>\n",
       "      <td>-1.340398</td>\n",
       "      <td>-17.511408</td>\n",
       "      <td>8.725648</td>\n",
       "      <td>-1.541163</td>\n",
       "      <td>...</td>\n",
       "      <td>14.519334</td>\n",
       "      <td>-2.414402</td>\n",
       "      <td>-3.053984</td>\n",
       "      <td>1.235405</td>\n",
       "      <td>-11.950718</td>\n",
       "      <td>-4.381988</td>\n",
       "      <td>-7.877891</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>-6.803874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1491226440</td>\n",
       "      <td>2364.8501</td>\n",
       "      <td>-5.170546</td>\n",
       "      <td>-6.793566</td>\n",
       "      <td>-11.43793</td>\n",
       "      <td>2.588127</td>\n",
       "      <td>-1.420398</td>\n",
       "      <td>-17.298508</td>\n",
       "      <td>8.725648</td>\n",
       "      <td>-1.381163</td>\n",
       "      <td>...</td>\n",
       "      <td>14.869334</td>\n",
       "      <td>-2.554402</td>\n",
       "      <td>-3.083984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.341988</td>\n",
       "      <td>-7.847891</td>\n",
       "      <td>0.271485</td>\n",
       "      <td>-6.943874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE      SP500  NASDAQ.AAL  NASDAQ.AAPL  NASDAQ.ADBE  NASDAQ.ADI  \\\n",
       "0  1491226200  2363.6101   -5.378346    -6.773566    -11.68793    2.593127   \n",
       "1  1491226260  2364.1001   -5.348346    -6.753566    -10.99793    2.633127   \n",
       "2  1491226320  2362.6799   -5.398346    -6.763466    -11.09293    2.583127   \n",
       "3  1491226380  2364.3101   -5.338346    -6.813566    -11.24503    2.553127   \n",
       "4  1491226440  2364.8501   -5.170546    -6.793566    -11.43793    2.588127   \n",
       "\n",
       "   NASDAQ.ADP  NASDAQ.ADSK  NASDAQ.AKAM  NASDAQ.ALXN   ...      NYSE.XEC  \\\n",
       "0   -1.250398   -17.778608     8.865648    -1.461163   ...     14.294334   \n",
       "1   -1.340398   -17.348608     8.945648    -1.501163   ...     14.294334   \n",
       "2   -1.267898   -17.488608     8.900648    -1.051163   ...     14.519334   \n",
       "3   -1.340398   -17.511408     8.725648    -1.541163   ...     14.519334   \n",
       "4   -1.420398   -17.298508     8.725648    -1.381163   ...     14.869334   \n",
       "\n",
       "   NYSE.XEL   NYSE.XL  NYSE.XOM   NYSE.XRX  NYSE.XYL  NYSE.YUM  NYSE.ZBH  \\\n",
       "0 -2.264402 -3.163984  1.245405 -11.940718 -4.321988 -7.897891  0.576485   \n",
       "1 -2.554402 -3.163984  1.245405 -11.920718 -4.321988 -8.017891  0.346485   \n",
       "2 -2.574402 -3.063984  1.235405 -11.940718 -4.421988 -8.007891  0.276485   \n",
       "3 -2.414402 -3.053984  1.235405 -11.950718 -4.381988 -7.877891  0.276485   \n",
       "4 -2.554402 -3.083984  1.245405 -11.940718 -4.341988 -7.847891  0.271485   \n",
       "\n",
       "   NYSE.ZTS  new_day  \n",
       "0 -6.833874      1.0  \n",
       "1 -6.833874      1.0  \n",
       "2 -6.818874      1.0  \n",
       "3 -6.803874      1.0  \n",
       "4 -6.943874      1.0  \n",
       "\n",
       "[5 rows x 503 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_scalled=Data_scalled.assign(new_day=new_day)\n",
    "Data_scalled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.0     391\n",
       "85.0     391\n",
       "87.0     391\n",
       "33.0     391\n",
       "5.0      391\n",
       "18.0     391\n",
       "14.0     391\n",
       "40.0     391\n",
       "73.0     391\n",
       "66.0     391\n",
       "29.0     391\n",
       "98.0     391\n",
       "62.0     391\n",
       "51.0     391\n",
       "95.0     391\n",
       "67.0     391\n",
       "11.0     391\n",
       "46.0     391\n",
       "36.0     391\n",
       "60.0     391\n",
       "81.0     391\n",
       "37.0     391\n",
       "31.0     391\n",
       "86.0     391\n",
       "84.0     391\n",
       "106.0    391\n",
       "47.0     391\n",
       "101.0    391\n",
       "76.0     391\n",
       "2.0      391\n",
       "        ... \n",
       "102.0    391\n",
       "58.0     391\n",
       "35.0     391\n",
       "63.0     391\n",
       "68.0     391\n",
       "94.0     391\n",
       "10.0     391\n",
       "6.0      391\n",
       "52.0     391\n",
       "24.0     391\n",
       "56.0     391\n",
       "48.0     391\n",
       "89.0     391\n",
       "41.0     391\n",
       "75.0     391\n",
       "34.0     391\n",
       "74.0     391\n",
       "54.0     391\n",
       "59.0     391\n",
       "69.0     391\n",
       "13.0     391\n",
       "50.0     391\n",
       "30.0     391\n",
       "72.0     391\n",
       "97.0     391\n",
       "45.0     391\n",
       "83.0     391\n",
       "19.0     391\n",
       "1.0      391\n",
       "64.0     211\n",
       "Name: new_day, Length: 106, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(Data_scalled['new_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above, we can see that day 64 has only 211 minutes data, so we will remove it just to be consistent across all days and input output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQ.AAL</th>\n",
       "      <th>NASDAQ.AAPL</th>\n",
       "      <th>NASDAQ.ADBE</th>\n",
       "      <th>NASDAQ.ADI</th>\n",
       "      <th>NASDAQ.ADP</th>\n",
       "      <th>NASDAQ.ADSK</th>\n",
       "      <th>NASDAQ.AKAM</th>\n",
       "      <th>NASDAQ.ALXN</th>\n",
       "      <th>...</th>\n",
       "      <th>NYSE.WYN</th>\n",
       "      <th>NYSE.XEC</th>\n",
       "      <th>NYSE.XEL</th>\n",
       "      <th>NYSE.XL</th>\n",
       "      <th>NYSE.XOM</th>\n",
       "      <th>NYSE.XRX</th>\n",
       "      <th>NYSE.XYL</th>\n",
       "      <th>NYSE.YUM</th>\n",
       "      <th>NYSE.ZBH</th>\n",
       "      <th>NYSE.ZTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1491226200</td>\n",
       "      <td>2363.6101</td>\n",
       "      <td>-5.378346</td>\n",
       "      <td>-6.773566</td>\n",
       "      <td>-11.68793</td>\n",
       "      <td>2.593127</td>\n",
       "      <td>-1.250398</td>\n",
       "      <td>-17.778608</td>\n",
       "      <td>8.865648</td>\n",
       "      <td>-1.461163</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.572211</td>\n",
       "      <td>14.294334</td>\n",
       "      <td>-2.264402</td>\n",
       "      <td>-3.163984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.321988</td>\n",
       "      <td>-7.897891</td>\n",
       "      <td>0.576485</td>\n",
       "      <td>-6.833874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1491226260</td>\n",
       "      <td>2364.1001</td>\n",
       "      <td>-5.348346</td>\n",
       "      <td>-6.753566</td>\n",
       "      <td>-10.99793</td>\n",
       "      <td>2.633127</td>\n",
       "      <td>-1.340398</td>\n",
       "      <td>-17.348608</td>\n",
       "      <td>8.945648</td>\n",
       "      <td>-1.501163</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.572211</td>\n",
       "      <td>14.294334</td>\n",
       "      <td>-2.554402</td>\n",
       "      <td>-3.163984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.920718</td>\n",
       "      <td>-4.321988</td>\n",
       "      <td>-8.017891</td>\n",
       "      <td>0.346485</td>\n",
       "      <td>-6.833874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1491226320</td>\n",
       "      <td>2362.6799</td>\n",
       "      <td>-5.398346</td>\n",
       "      <td>-6.763466</td>\n",
       "      <td>-11.09293</td>\n",
       "      <td>2.583127</td>\n",
       "      <td>-1.267898</td>\n",
       "      <td>-17.488608</td>\n",
       "      <td>8.900648</td>\n",
       "      <td>-1.051163</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.357211</td>\n",
       "      <td>14.519334</td>\n",
       "      <td>-2.574402</td>\n",
       "      <td>-3.063984</td>\n",
       "      <td>1.235405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.421988</td>\n",
       "      <td>-8.007891</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>-6.818874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1491226380</td>\n",
       "      <td>2364.3101</td>\n",
       "      <td>-5.338346</td>\n",
       "      <td>-6.813566</td>\n",
       "      <td>-11.24503</td>\n",
       "      <td>2.553127</td>\n",
       "      <td>-1.340398</td>\n",
       "      <td>-17.511408</td>\n",
       "      <td>8.725648</td>\n",
       "      <td>-1.541163</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.482211</td>\n",
       "      <td>14.519334</td>\n",
       "      <td>-2.414402</td>\n",
       "      <td>-3.053984</td>\n",
       "      <td>1.235405</td>\n",
       "      <td>-11.950718</td>\n",
       "      <td>-4.381988</td>\n",
       "      <td>-7.877891</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>-6.803874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1491226440</td>\n",
       "      <td>2364.8501</td>\n",
       "      <td>-5.170546</td>\n",
       "      <td>-6.793566</td>\n",
       "      <td>-11.43793</td>\n",
       "      <td>2.588127</td>\n",
       "      <td>-1.420398</td>\n",
       "      <td>-17.298508</td>\n",
       "      <td>8.725648</td>\n",
       "      <td>-1.381163</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.472211</td>\n",
       "      <td>14.869334</td>\n",
       "      <td>-2.554402</td>\n",
       "      <td>-3.083984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.341988</td>\n",
       "      <td>-7.847891</td>\n",
       "      <td>0.271485</td>\n",
       "      <td>-6.943874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE      SP500  NASDAQ.AAL  NASDAQ.AAPL  NASDAQ.ADBE  NASDAQ.ADI  \\\n",
       "0  1491226200  2363.6101   -5.378346    -6.773566    -11.68793    2.593127   \n",
       "1  1491226260  2364.1001   -5.348346    -6.753566    -10.99793    2.633127   \n",
       "2  1491226320  2362.6799   -5.398346    -6.763466    -11.09293    2.583127   \n",
       "3  1491226380  2364.3101   -5.338346    -6.813566    -11.24503    2.553127   \n",
       "4  1491226440  2364.8501   -5.170546    -6.793566    -11.43793    2.588127   \n",
       "\n",
       "   NASDAQ.ADP  NASDAQ.ADSK  NASDAQ.AKAM  NASDAQ.ALXN    ...      NYSE.WYN  \\\n",
       "0   -1.250398   -17.778608     8.865648    -1.461163    ...    -13.572211   \n",
       "1   -1.340398   -17.348608     8.945648    -1.501163    ...    -13.572211   \n",
       "2   -1.267898   -17.488608     8.900648    -1.051163    ...    -13.357211   \n",
       "3   -1.340398   -17.511408     8.725648    -1.541163    ...    -13.482211   \n",
       "4   -1.420398   -17.298508     8.725648    -1.381163    ...    -13.472211   \n",
       "\n",
       "    NYSE.XEC  NYSE.XEL   NYSE.XL  NYSE.XOM   NYSE.XRX  NYSE.XYL  NYSE.YUM  \\\n",
       "0  14.294334 -2.264402 -3.163984  1.245405 -11.940718 -4.321988 -7.897891   \n",
       "1  14.294334 -2.554402 -3.163984  1.245405 -11.920718 -4.321988 -8.017891   \n",
       "2  14.519334 -2.574402 -3.063984  1.235405 -11.940718 -4.421988 -8.007891   \n",
       "3  14.519334 -2.414402 -3.053984  1.235405 -11.950718 -4.381988 -7.877891   \n",
       "4  14.869334 -2.554402 -3.083984  1.245405 -11.940718 -4.341988 -7.847891   \n",
       "\n",
       "   NYSE.ZBH  NYSE.ZTS  \n",
       "0  0.576485 -6.833874  \n",
       "1  0.346485 -6.833874  \n",
       "2  0.276485 -6.818874  \n",
       "3  0.276485 -6.803874  \n",
       "4  0.271485 -6.943874  \n",
       "\n",
       "[5 rows x 502 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_scalled1=Data_scalled[new_day!=64]\n",
    "del Data_scalled1['new_day']\n",
    "Data_scalled1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>SP500</th>\n",
       "      <th>NASDAQ.AAL</th>\n",
       "      <th>NASDAQ.AAPL</th>\n",
       "      <th>NASDAQ.ADBE</th>\n",
       "      <th>NASDAQ.ADI</th>\n",
       "      <th>NASDAQ.ADP</th>\n",
       "      <th>NASDAQ.ADSK</th>\n",
       "      <th>NASDAQ.AKAM</th>\n",
       "      <th>NASDAQ.ALXN</th>\n",
       "      <th>...</th>\n",
       "      <th>NYSE.XEC</th>\n",
       "      <th>NYSE.XEL</th>\n",
       "      <th>NYSE.XL</th>\n",
       "      <th>NYSE.XOM</th>\n",
       "      <th>NYSE.XRX</th>\n",
       "      <th>NYSE.XYL</th>\n",
       "      <th>NYSE.YUM</th>\n",
       "      <th>NYSE.ZBH</th>\n",
       "      <th>NYSE.ZTS</th>\n",
       "      <th>new_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1491226200</td>\n",
       "      <td>2363.6101</td>\n",
       "      <td>-5.378346</td>\n",
       "      <td>-6.773566</td>\n",
       "      <td>-11.68793</td>\n",
       "      <td>2.593127</td>\n",
       "      <td>-1.250398</td>\n",
       "      <td>-17.778608</td>\n",
       "      <td>8.865648</td>\n",
       "      <td>-1.461163</td>\n",
       "      <td>...</td>\n",
       "      <td>14.294334</td>\n",
       "      <td>-2.264402</td>\n",
       "      <td>-3.163984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.321988</td>\n",
       "      <td>-7.897891</td>\n",
       "      <td>0.576485</td>\n",
       "      <td>-6.833874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1491226260</td>\n",
       "      <td>2364.1001</td>\n",
       "      <td>-5.348346</td>\n",
       "      <td>-6.753566</td>\n",
       "      <td>-10.99793</td>\n",
       "      <td>2.633127</td>\n",
       "      <td>-1.340398</td>\n",
       "      <td>-17.348608</td>\n",
       "      <td>8.945648</td>\n",
       "      <td>-1.501163</td>\n",
       "      <td>...</td>\n",
       "      <td>14.294334</td>\n",
       "      <td>-2.554402</td>\n",
       "      <td>-3.163984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.920718</td>\n",
       "      <td>-4.321988</td>\n",
       "      <td>-8.017891</td>\n",
       "      <td>0.346485</td>\n",
       "      <td>-6.833874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1491226320</td>\n",
       "      <td>2362.6799</td>\n",
       "      <td>-5.398346</td>\n",
       "      <td>-6.763466</td>\n",
       "      <td>-11.09293</td>\n",
       "      <td>2.583127</td>\n",
       "      <td>-1.267898</td>\n",
       "      <td>-17.488608</td>\n",
       "      <td>8.900648</td>\n",
       "      <td>-1.051163</td>\n",
       "      <td>...</td>\n",
       "      <td>14.519334</td>\n",
       "      <td>-2.574402</td>\n",
       "      <td>-3.063984</td>\n",
       "      <td>1.235405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.421988</td>\n",
       "      <td>-8.007891</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>-6.818874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1491226380</td>\n",
       "      <td>2364.3101</td>\n",
       "      <td>-5.338346</td>\n",
       "      <td>-6.813566</td>\n",
       "      <td>-11.24503</td>\n",
       "      <td>2.553127</td>\n",
       "      <td>-1.340398</td>\n",
       "      <td>-17.511408</td>\n",
       "      <td>8.725648</td>\n",
       "      <td>-1.541163</td>\n",
       "      <td>...</td>\n",
       "      <td>14.519334</td>\n",
       "      <td>-2.414402</td>\n",
       "      <td>-3.053984</td>\n",
       "      <td>1.235405</td>\n",
       "      <td>-11.950718</td>\n",
       "      <td>-4.381988</td>\n",
       "      <td>-7.877891</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>-6.803874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1491226440</td>\n",
       "      <td>2364.8501</td>\n",
       "      <td>-5.170546</td>\n",
       "      <td>-6.793566</td>\n",
       "      <td>-11.43793</td>\n",
       "      <td>2.588127</td>\n",
       "      <td>-1.420398</td>\n",
       "      <td>-17.298508</td>\n",
       "      <td>8.725648</td>\n",
       "      <td>-1.381163</td>\n",
       "      <td>...</td>\n",
       "      <td>14.869334</td>\n",
       "      <td>-2.554402</td>\n",
       "      <td>-3.083984</td>\n",
       "      <td>1.245405</td>\n",
       "      <td>-11.940718</td>\n",
       "      <td>-4.341988</td>\n",
       "      <td>-7.847891</td>\n",
       "      <td>0.271485</td>\n",
       "      <td>-6.943874</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE      SP500  NASDAQ.AAL  NASDAQ.AAPL  NASDAQ.ADBE  NASDAQ.ADI  \\\n",
       "0  1491226200  2363.6101   -5.378346    -6.773566    -11.68793    2.593127   \n",
       "1  1491226260  2364.1001   -5.348346    -6.753566    -10.99793    2.633127   \n",
       "2  1491226320  2362.6799   -5.398346    -6.763466    -11.09293    2.583127   \n",
       "3  1491226380  2364.3101   -5.338346    -6.813566    -11.24503    2.553127   \n",
       "4  1491226440  2364.8501   -5.170546    -6.793566    -11.43793    2.588127   \n",
       "\n",
       "   NASDAQ.ADP  NASDAQ.ADSK  NASDAQ.AKAM  NASDAQ.ALXN   ...      NYSE.XEC  \\\n",
       "0   -1.250398   -17.778608     8.865648    -1.461163   ...     14.294334   \n",
       "1   -1.340398   -17.348608     8.945648    -1.501163   ...     14.294334   \n",
       "2   -1.267898   -17.488608     8.900648    -1.051163   ...     14.519334   \n",
       "3   -1.340398   -17.511408     8.725648    -1.541163   ...     14.519334   \n",
       "4   -1.420398   -17.298508     8.725648    -1.381163   ...     14.869334   \n",
       "\n",
       "   NYSE.XEL   NYSE.XL  NYSE.XOM   NYSE.XRX  NYSE.XYL  NYSE.YUM  NYSE.ZBH  \\\n",
       "0 -2.264402 -3.163984  1.245405 -11.940718 -4.321988 -7.897891  0.576485   \n",
       "1 -2.554402 -3.163984  1.245405 -11.920718 -4.321988 -8.017891  0.346485   \n",
       "2 -2.574402 -3.063984  1.235405 -11.940718 -4.421988 -8.007891  0.276485   \n",
       "3 -2.414402 -3.053984  1.235405 -11.950718 -4.381988 -7.877891  0.276485   \n",
       "4 -2.554402 -3.083984  1.245405 -11.940718 -4.341988 -7.847891  0.271485   \n",
       "\n",
       "   NYSE.ZTS  new_day  \n",
       "0 -6.833874      1.0  \n",
       "1 -6.833874      1.0  \n",
       "2 -6.818874      1.0  \n",
       "3 -6.803874      1.0  \n",
       "4 -6.943874      1.0  \n",
       "\n",
       "[5 rows x 503 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will re-run the number of days in the data\n",
    "new_day=np.zeros(shape=(41055,1))\n",
    "new_day[0,0]=1\n",
    "values=Data_scalled1['DATE'].values\n",
    "for i in range(41054):\n",
    "    if values[i+1]==values[i]+60:\n",
    "        new_day[i+1]=new_day[i]\n",
    "    else:\n",
    "        new_day[i+1]=new_day[i]+1\n",
    "\n",
    "\n",
    "Data_scalled1=Data_scalled1.assign(new_day=new_day)\n",
    "Data_scalled1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names=Data_scalled1.columns.get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41055, 503)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_scalled1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Due to resources constraints we are not able to use the whole data (53,106 images) to train the model, the maximum capabilities will be used and will try in the future to run it on more powerful computer or on a cloud once the setup is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width=96\n",
    "nb_stocks=50\n",
    "nb_days=100\n",
    "nb_input=293 #this is the percentage of each day data that will be used for predecting the remaining part of the day\n",
    "images=np.zeros((nb_stocks*nb_days, width, width, 3), dtype=np.uint8)\n",
    "stock_return=np.zeros((nb_stocks*nb_days,1))\n",
    "stock_return_30min=np.zeros((nb_stocks*nb_days,1))\n",
    "for i in range(nb_stocks):\n",
    "    for j in range(nb_days):\n",
    "#        plt.figure(figsize=(5,5))\n",
    "#        plt.plot(Data_scalled1[names[i+1]][Data_scalled1['new_day']==j+1][0:nb_input])\n",
    "#        plt.axis('off')\n",
    "#        plt.savefig('stock'+names[i+1]+str(j)+'.png')\n",
    "#        Image.open('stock'+names[i+1]+str(j)+'.png').save('stock'+names[i+1]+str(j)+'.png')\n",
    "        images[nb_days*i+j]=cv2.resize(cv2.imread('C:/Users/User/Documents/Data Scientist - Harvard/Advanced Machine Learning/Final Project/'+'stock'+names[i+1]+str(j)+'.png'),(width,width))\n",
    "        stock_return[nb_days*i+j]=Data_scalled1[names[i+1]][Data_scalled1['new_day']==j+1].values[390]/Data_scalled1[names[i+1]][Data_scalled1['new_day']==j+1].values[nb_input]-1 # this is the return by doing future price over current price minus\n",
    "        stock_return_30min[nb_days*i+j]=Data_scalled1[names[i+1]][Data_scalled1['new_day']==j+1].values[323]/Data_scalled1[names[i+1]][Data_scalled1['new_day']==j+1].values[nb_input]-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I only imported data for 50 stocks on 100 days, then i commented the plotting code to reduce memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del Data_scalled \n",
    "del data \n",
    "del values\n",
    "del new_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZKpbEuiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c19b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1][:,:,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 96, 96, 3)\n",
      "(5000, 96, 96)\n",
      "(5000, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "#Since colors is not an important variable here, I will average across channels to get one channel only\n",
    "images1=np.mean(images,axis=3)\n",
    "print(images.shape)\n",
    "print(images1.shape)\n",
    "\n",
    "from numpy import newaxis\n",
    "images1=images1[:,:,:,newaxis]\n",
    "print(images1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting the data into training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, valid_data, train_labels, valid_labels = train_test_split(images1, stock_return, test_size=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Some stocks shows an outlier by generating returns more than 100%, the next code will remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stock_return_f=stock_return[abs(stock_return)<1][:,newaxis]\n",
    "stock_return_30min_f=stock_return_30min[abs(stock_return)<1][:,newaxis]\n",
    "stock_return_f.shape\n",
    "images_f=np.zeros((stock_return_f.shape[0], width, width, 1), dtype=np.uint8)\n",
    "j=0\n",
    "for i in range(images1.shape[0]):\n",
    "    if abs(stock_return[i])<1:\n",
    "        images_f[j]=images1[i]\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4774, 1)\n",
      "(4774, 96, 96, 1)\n",
      "(4774, 1)\n"
     ]
    }
   ],
   "source": [
    "print(stock_return_f.shape)\n",
    "print(images_f.shape)\n",
    "print(stock_return_30min_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "flattened_size = images_f.reshape((-1, width * width)).astype(np.float32).shape[1]\n",
    "hidden_nodes = 10\n",
    "lamb_reg = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, flattened_size), name=\"TrainingData\")\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, shape=(batch_size, 1), name=\"TrainingLabels\")\n",
    "    #tf_valid_dataset = tf.constant(valid_data.reshape((-1, width * width*3)).astype(np.float32), name=\"ValidationData\")\n",
    "    #tf_test_dataset = tf.constant(test_data.reshape((-1, width * width*3)).astype(np.float32), name=\"TestingData\")\n",
    "    \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([flattened_size, hidden_nodes]), name=\"weights1\")\n",
    "    layer1_biases = tf.Variable(tf.zeros([hidden_nodes]), name=\"biases1\")\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([hidden_nodes, 1]), name=\"weights2\")\n",
    "    layer2_biases = tf.Variable(tf.ones([1]), name=\"biases2\")\n",
    "\n",
    "    # Model.\n",
    "    def model(data, name):\n",
    "        with tf.name_scope(name) as scope:\n",
    "            layer1 = tf.add(tf.matmul(data, layer1_weights), layer1_biases, name=\"layer1\")\n",
    "            hidden1 = tf.nn.relu(layer1, name=\"relu1\")\n",
    "            layer2 = tf.add(tf.matmul(hidden1, layer2_weights), layer2_biases, name=\"layer2\")\n",
    "            return layer2 \n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, name=\"logits\")\n",
    "    loss = tf.nn.l2_loss(logits-tf_train_labelset, name=\"loss\")\n",
    "    regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) + \n",
    "                    tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases))\n",
    "    \n",
    "    # Add the regularization term to the loss.\n",
    "    loss += lamb_reg * regularizers\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits\n",
    "    #valid_prediction = tf.nn.softmax(model(tf_valid_dataset, name=\"validation\"))\n",
    "    #test_prediction = tf.nn.softmax(model(tf_test_dataset, name=\"testing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at epoch 0: 2916176640.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at epoch 50: nan\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at epoch 100: nan\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at epoch 150: nan\n",
      "Minibatch accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def run_session(num_epochs, name):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run() \n",
    "        merged = tf.summary.merge_all()  \n",
    "        writer = tf.summary.FileWriter(\"/tmp/tensorflowlogs\", session.graph)\n",
    "        print(\"Initialized\")\n",
    "        for epoch in range(num_epochs):\n",
    "            offset = (epoch * batch_size) % (stock_return_f.shape[0] - batch_size)\n",
    "            batch_data = images_f.reshape((-1, width * width)).astype(np.float32)[offset:(offset + batch_size), :]\n",
    "            batch_labels = stock_return_f[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labelset : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (epoch % 50 == 0):\n",
    "                print('Minibatch loss at epoch %d: %f' % (epoch, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                #print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "        #test_preds = pd.DataFrame(test_prediction.eval().ravel(), columns=[name])\n",
    "\n",
    "run_session(200, \"H1L_NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above we have generated the return for 25% of the day and for 30 minutes. We will try to predict both of them and see the effect of time horizon on the quality of forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CNN based on LeNet-5 - 25% of the day return (end of day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])\n",
    "\n",
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=1e-4)\n",
    "    #initial = tf.truncated_normal(shape, stddev=tf.sqrt(2.0/shape[0]))\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.zeros(shape)\n",
    "    #initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name)\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "normalize_simple = lambda dataset: ((dataset / 255.0) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 256]\n"
     ]
    }
   ],
   "source": [
    "height = width\n",
    "channels = 1\n",
    "num_labels = 1\n",
    "\n",
    "#valid_batch_size = 5000\n",
    "#test_batch_size = 6508\n",
    "batch_size = 256\n",
    "patch_size = 1\n",
    "depth1 = 2\n",
    "depth2 =4\n",
    "num_hidden = 256\n",
    "lamb_reg = 0.5\n",
    "learning_rate = 0.01  #  learning rate for the momentum optimizer\n",
    "\n",
    "reset_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, \n",
    "                                      shape=(batch_size, height, width, channels), name=\"TrainingData\")\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, \n",
    "                                     shape=(batch_size, num_labels), name=\"TrainingLabels\")\n",
    "    #tf_valid_dataset = tf.placeholder(tf.float32,\n",
    "    #                                 shape=(valid_batch_size, height, width, channels), name=\"ValidationData\")\n",
    "    #tf_test_dataset = tf.placeholder(tf.float32,\n",
    "     #                                shape=(test_batch_size, height, width, channels), name=\"TestingData\")\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "    # Variables \n",
    "    layer1_weights = weight_variable([patch_size, patch_size, channels, depth1], name=\"weights1\")\n",
    "    layer1_biases = bias_variable([depth1], name=\"biases1\")\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth1, depth2], name=\"weights2\")\n",
    "    layer2_biases = bias_variable([depth2], name=\"biases2\")    \n",
    "    layer3_weights = weight_variable([height // 4 * width // 4 * depth2, num_hidden], name=\"weights3\")\n",
    "    layer3_biases = bias_variable([num_hidden], name=\"biases3\")    # Model with dropout\n",
    "    layer4_weights = weight_variable([num_hidden, num_labels], name=\"weights4\")\n",
    "    layer4_biases = bias_variable([num_labels], name=\"biases4\")\n",
    "            \n",
    "    def model(data, name, proba=keep_prob):\n",
    "            # Model.\n",
    "            inputs = tf.cast(normalize_simple(data),dtype=tf.float32)\n",
    "            #inputs = tf.cast(data,dtype=tf.float32)\n",
    "            #inputs = LeCunLCN(data, data.get_shape().as_list())\n",
    "            # Convolution\n",
    "            conv_1 = tf.nn.bias_add(tf.nn.conv2d(inputs, layer1_weights, [1, 1, 1, 1], padding='SAME'), layer1_biases, name=\"layer1\")\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(conv_1), ksize=[1, 2, 2, 1],\n",
    "                                     strides=[1, 2, 2, 1], padding='SAME', name=\"pooled1\")\n",
    "            # Convolution\n",
    "            conv_2 = tf.nn.bias_add(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME'), layer2_biases, name=\"layer2\")\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(conv_2), ksize=[1, 2, 2, 1],\n",
    "                                     strides=[1, 2, 2, 1], padding='SAME', name=\"pooled2\")\n",
    "            # Fully Connected Layer\n",
    "            shape = pool_2.get_shape().as_list()\n",
    "            #print (shape)\n",
    "            reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            full_3 = tf.nn.bias_add(tf.matmul(reshape, layer3_weights),layer3_biases)\n",
    "            \n",
    "            # Dropout\n",
    "            full_4 = tf.nn.dropout(full_3, proba)\n",
    "            print(full_4.get_shape().as_list())\n",
    "            layer_fc = tf.nn.bias_add(tf.matmul(full_4, layer4_weights),layer4_biases, name=\"layer_fc\")\n",
    "            return layer_fc\n",
    "  \n",
    "   # Training computation.\n",
    "    logits = model(tf_train_dataset, \"logits\", keep_prob)\n",
    "    loss=tf.nn.l2_loss(logits-tf_train_labelset, name=\"loss\")\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = tf_train_labelset), name=\"loss\")\n",
    "    #regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "     #               tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "     #               tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "     #               tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases) )\n",
    "\n",
    "    # Add the regularization term to the loss.\n",
    "    #loss += lamb_reg * regularizers\n",
    "    #loss = tf.reduce_mean(loss + lamb_reg * regularizers)\n",
    "    \n",
    "    # Optimizer\n",
    "    #global_step = tf.Variable(0, name=\"globalstep\")  # count  number of steps taken.\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9).minimize(loss, global_step=global_step)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits  #tf.nn.softmax(logits)\n",
    "    #valid_prediction = tf.nn.softmax(model(tf_valid_dataset, \"validation\", 1.0))  # no dropout\n",
    "    #test_prediction = tf.nn.softmax(model(tf_test_dataset, \"testing\", 1.0))  # no dropout\n",
    "    saver = tf.train.Saver()   # a saver variable to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Session function derived from that of last Section\n",
    "# Specify the number of epochs, name to give the model and the dropout probability\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "def run_session(num_epochs, name, k_prob):\n",
    "    \n",
    "    with tf.Session(config=config, graph=graph) as session:\n",
    "            tf.global_variables_initializer().run() \n",
    "            loss_summary = tf.summary.scalar('Loss', loss)\n",
    "            learning_rate_summary = tf.summary.scalar('Learning_rate', learning_rate)\n",
    "            merged = tf.summary.merge_all()  \n",
    "            writer = tf.summary.FileWriter(\"/tmp/tensorflowlogs\", session.graph)\n",
    "            print(\"Initialized\\n\")\n",
    "            epoch = 0\n",
    "            for step in range(int(stock_return_f.shape[0]/batch_size)*num_epochs):\n",
    "                #epoch = int(20*train_labels.shape[0]/batch_size)\n",
    "                offset = (step * batch_size) % (stock_return_f.shape[0] - batch_size)\n",
    "                batch_data = images_f[offset:(offset + batch_size)]\n",
    "                batch_labels = stock_return_f[offset:(offset + batch_size)]\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labelset : batch_labels, keep_prob : k_prob, training: True}\n",
    "                _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "                writer.add_summary(loss_summary.eval(feed_dict=feed_dict), epoch)\n",
    "                writer.add_summary(learning_rate_summary.eval(), epoch)\n",
    "                if (offset < batch_size):\n",
    "                    epoch += 1\n",
    "                    if (epoch % 1) == 0:\n",
    "                        print(\"Minibatch loss at epoch {}: {}\".format(epoch, l))\n",
    "                        #print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "                        true_preds = 0\n",
    "                        #for batch_num in range(int(valid_labels.shape[0]/valid_batch_size)): \n",
    "                            #batch_valid_data = valid_data[batch_num*valid_batch_size:(batch_num+1)*valid_batch_size]\n",
    "                            #batch_valid_labels = valid_labels[batch_num*valid_batch_size:(batch_num+1)*valid_batch_size]\n",
    "                         #   feed_dict = {tf_valid_dataset : batch_valid_data}\n",
    "                         #   predictions = session.run(valid_prediction, feed_dict) \n",
    "                          #  true_preds += np.sum(np.argmax(predictions, 1) == np.argmax(batch_valid_labels, 1))\n",
    "                        #print(\"Validation accuracy: {:.1f}\\n\".format(100.0 * true_preds / valid_data.shape[0]))\n",
    "\n",
    "            save_path = saver.save(session, \"/tmp/tensorflowmodels/\" + name +\".ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Minibatch loss at epoch 1: 1.4030494689941406\n",
      "Minibatch loss at epoch 2: 3.2261619567871094\n",
      "Minibatch loss at epoch 3: 4.837370872497559\n",
      "Minibatch loss at epoch 4: 1.5580404996871948\n",
      "Minibatch loss at epoch 5: 4.535417079925537\n",
      "Minibatch loss at epoch 6: 5.987335205078125\n",
      "Minibatch loss at epoch 7: 2.3854758739471436\n",
      "Minibatch loss at epoch 8: 4.511142730712891\n",
      "Minibatch loss at epoch 9: 6.2186279296875\n",
      "Minibatch loss at epoch 10: 2.451119899749756\n",
      "Minibatch loss at epoch 11: 4.4771728515625\n",
      "Minibatch loss at epoch 12: 6.680111885070801\n",
      "Minibatch loss at epoch 13: 2.456920623779297\n",
      "Minibatch loss at epoch 14: 4.418601989746094\n",
      "Minibatch loss at epoch 15: 6.851040363311768\n",
      "Minibatch loss at epoch 16: 2.4655182361602783\n",
      "Minibatch loss at epoch 17: 4.256555557250977\n",
      "Minibatch loss at epoch 18: 6.8745927810668945\n",
      "Minibatch loss at epoch 19: 2.8897364139556885\n",
      "Minibatch loss at epoch 20: 4.584770202636719\n",
      "Minibatch loss at epoch 21: 1.4404199123382568\n",
      "Minibatch loss at epoch 22: 3.5593338012695312\n",
      "Minibatch loss at epoch 23: 5.906700134277344\n",
      "Minibatch loss at epoch 24: 2.37191104888916\n",
      "Minibatch loss at epoch 25: 4.508279323577881\n",
      "Minibatch loss at epoch 26: 6.120709419250488\n",
      "Minibatch loss at epoch 27: 2.443819999694824\n",
      "Minibatch loss at epoch 28: 4.524941444396973\n",
      "Minibatch loss at epoch 29: 6.595844268798828\n",
      "Minibatch loss at epoch 30: 2.453601360321045\n",
      "Minibatch loss at epoch 31: 4.190612316131592\n",
      "Minibatch loss at epoch 32: 6.853445529937744\n",
      "Minibatch loss at epoch 33: 2.4603567123413086\n",
      "Minibatch loss at epoch 34: 4.294350624084473\n",
      "Minibatch loss at epoch 35: 6.907793045043945\n",
      "Minibatch loss at epoch 36: 2.692683696746826\n",
      "Minibatch loss at epoch 37: 4.512562274932861\n",
      "Minibatch loss at epoch 38: 1.412567138671875\n",
      "Minibatch loss at epoch 39: 3.271822929382324\n",
      "Minibatch loss at epoch 40: 5.0321125984191895\n",
      "Minibatch loss at epoch 41: 1.773399829864502\n",
      "Minibatch loss at epoch 42: 4.537612438201904\n",
      "Minibatch loss at epoch 43: 6.04817533493042\n",
      "Minibatch loss at epoch 44: 2.4288690090179443\n",
      "Minibatch loss at epoch 45: 4.517089366912842\n",
      "Minibatch loss at epoch 46: 6.286728382110596\n",
      "Minibatch loss at epoch 47: 2.4519848823547363\n",
      "Minibatch loss at epoch 48: 4.473062992095947\n",
      "Minibatch loss at epoch 49: 6.720646381378174\n",
      "Minibatch loss at epoch 50: 2.4574978351593018\n",
      "Minibatch loss at epoch 51: 4.407191276550293\n",
      "Minibatch loss at epoch 52: 6.863816738128662\n",
      "Minibatch loss at epoch 53: 2.4750072956085205\n",
      "Minibatch loss at epoch 54: 4.198241233825684\n",
      "Minibatch loss at epoch 55: 6.871846675872803\n",
      "Minibatch loss at epoch 56: 3.0774686336517334\n",
      "Minibatch loss at epoch 57: 4.590533256530762\n",
      "Minibatch loss at epoch 58: 1.4411494731903076\n",
      "Minibatch loss at epoch 59: 3.9798948764801025\n",
      "Minibatch loss at epoch 60: 5.866605758666992\n",
      "Minibatch loss at epoch 61: 2.3761253356933594\n",
      "Minibatch loss at epoch 62: 4.509274482727051\n",
      "Minibatch loss at epoch 63: 6.123478412628174\n",
      "Minibatch loss at epoch 64: 2.4438109397888184\n",
      "Minibatch loss at epoch 65: 4.500997066497803\n",
      "Minibatch loss at epoch 66: 6.5990447998046875\n",
      "Minibatch loss at epoch 67: 2.455458641052246\n",
      "Minibatch loss at epoch 68: 4.401381015777588\n",
      "Minibatch loss at epoch 69: 6.830785751342773\n",
      "Minibatch loss at epoch 70: 2.461454153060913\n",
      "Minibatch loss at epoch 71: 4.293818950653076\n",
      "Minibatch loss at epoch 72: 6.877449989318848\n",
      "Minibatch loss at epoch 73: 2.696767568588257\n",
      "Minibatch loss at epoch 74: 4.541139602661133\n",
      "Minibatch loss at epoch 75: 1.421403408050537\n",
      "Minibatch loss at epoch 76: 3.522568702697754\n",
      "Minibatch loss at epoch 77: 5.152289390563965\n",
      "Minibatch loss at epoch 78: 1.8433785438537598\n",
      "Minibatch loss at epoch 79: 4.516969680786133\n",
      "Minibatch loss at epoch 80: 6.049760818481445\n",
      "Minibatch loss at epoch 81: 2.4288926124572754\n",
      "Minibatch loss at epoch 82: 4.523351669311523\n",
      "Minibatch loss at epoch 83: 6.452700138092041\n",
      "Minibatch loss at epoch 84: 2.4526195526123047\n",
      "Minibatch loss at epoch 85: 4.462158203125\n",
      "Minibatch loss at epoch 86: 6.723970890045166\n",
      "Minibatch loss at epoch 87: 2.4581139087677\n",
      "Minibatch loss at epoch 88: 4.3388776779174805\n",
      "Minibatch loss at epoch 89: 6.904872894287109\n",
      "Minibatch loss at epoch 90: 2.4785046577453613\n",
      "Minibatch loss at epoch 91: 4.201384544372559\n",
      "Minibatch loss at epoch 92: 6.862571716308594\n",
      "Minibatch loss at epoch 93: 3.089672327041626\n",
      "Minibatch loss at epoch 94: 4.9599504470825195\n",
      "Minibatch loss at epoch 95: 1.4516665935516357\n",
      "Minibatch loss at epoch 96: 4.452073574066162\n",
      "Minibatch loss at epoch 97: 5.9401326179504395\n",
      "Minibatch loss at epoch 98: 2.3822031021118164\n",
      "Minibatch loss at epoch 99: 4.504121780395508\n",
      "Minibatch loss at epoch 100: 6.181526184082031\n",
      "Minibatch loss at epoch 101: 2.4441471099853516\n",
      "Minibatch loss at epoch 102: 4.491710662841797\n",
      "Model saved in file: /tmp/tensorflowmodels/LeNet_CNN.ckpt\n"
     ]
    }
   ],
   "source": [
    "run_session(100, \"LeNet_CNN\", 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tensorflowmodels/LeNet_CNN.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEu9JREFUeJzt3X+MHGd9x/H3N2eHHi3UCTEh/oWDFKw6TanJNUGCqhQS\nnBioTWnVBAqhLbIiEUSr1q2jpC0S/AFYrQA1YLk0alJooqoJxkKmB0kbqoo6xCZpHJMecYKIczaJ\niZqCwErs87d/3NjsXXfv5m7X++Oe90taeeaZZ/b53mi8n9tndm4jM5EkleesXhcgSeoNA0CSCmUA\nSFKhDABJKpQBIEmFMgAkqVAGgCQVygCQpEIZAJJUqEW9LmAm5513Xq5evbrXZUjSwNi3b98PMnNp\nnb59HQCrV69m7969vS5DkgZGRHyvbl+ngCSpUAaAJBXKAJCkQhkAklQoA0CSCtWRAIiIqyJiLCIO\nRsTWFn3eGBEPRcSBiPh6J8aVJM1f2x8DjYgh4BbgSuAp4IGI2JWZ327oswT4DHBVZj4ZES9vd1xJ\nUns68Q7gMuBgZj6RmS8AdwIbp/V5F3B3Zj4JkJnPdGBcSVIbOnEj2HLgUMP6U8Dl0/q8GlgcEfcB\nLwE+lZm3d2Bsqat2PjjOttExDj93jGVLhtmyfg2b1i3vdVnSvHTrTuBFwKXAm4Fh4D8jYk9mfmd6\nx4jYDGwGWLVqVZfKk2a388Fxbrx7P8eOTwAw/twxbrx7P4AhoIHUiSmgcWBlw/qKqq3RU8BoZv44\nM38A/DvwmmZPlpk7MnMkM0eWLq315yykrtg2Onb6xf+UY8cn2DY61qOKpPZ0IgAeAC6KiAsj4mzg\nGmDXtD5fAt4QEYsi4sVMThE92oGxpa45/NyxObVL/a7tKaDMPBERNwCjwBBwa2YeiIjrq+3bM/PR\niPgX4GHgJPC5zHyk3bGlblq2ZJjxJi/2y5YM96AaqX0duQaQmbuB3dPatk9b3wZs68R4Ui9sWb9m\nyjUAgOHFQ2xZv6aHVUnz19d/DlrqJ6cu9PopIC0UBoA0B5vWLfcFXwuGfwtIkgplAEhSoQwASSqU\nASBJhTIAJKlQBoAkFcoAkKRCGQCSVCgDQJIKZQBIUqEMAEkqlAEgSYUyACSpUP41UGkO/FJ4LSQG\ngFSTXwqvhcYpIKkmvxReC40BINXkl8JroTEApJpaffm7XwqvQWUASDVtWb+G4cVDU9r8UngNMi8C\nSzX5pfBaaAwAaQ78UngtJE4BSVKhDABJKpRTQNIceCewFhIDQKrJO4G10DgFJNXkncBaaDoSABFx\nVUSMRcTBiNg6Q79fiYgTEfFbnRhX6ibvBNZC03YARMQQcAtwNbAWuDYi1rbo93Hgq+2OKfWCdwJr\noenEO4DLgIOZ+URmvgDcCWxs0u+DwF3AMx0YU+q6LevXsHgoprQtHgrvBNbA6kQALAcONaw/VbWd\nFhHLgXcAn+3AeFLv5Czr0gDp1kXgTwJ/lpknZ+sYEZsjYm9E7D169GgXSpPq2TY6xvGTU1/xj59M\nLwJrYHXiY6DjwMqG9RVVW6MR4M6IADgP2BARJzJz5/Qny8wdwA6AkZERf79S3/AisBaaTgTAA8BF\nEXEhky/81wDvauyQmReeWo6Ivwe+3OzFX+pny5YMM97kxd6LwBpUbU8BZeYJ4AZgFHgU+KfMPBAR\n10fE9e0+v9Qvtqxfw+Kzpl0EPsuLwBpcHbkTODN3A7untW1v0fd9nRhT6omYZV0aIN4JLNW0bXSM\n4xPTLgJPeBFYg8sAkGpqNv8/U7vU7wwAqaahaD7f06pd6ncGgFTTRDb/VHKrdqnfGQBSTctbfNyz\nVbvU7wwAqaYt69cwvHhoStvw4iE/BqqB5RfCSDWd+tIXvxFMC4UBIM3BpnXLfcHXguEUkCQVygCQ\npEIZAJJUKANAkgplAEhSoQwASSqUASBJhTIAJKlQBoAkFcoAkKRCGQCSVCgDQJIK5R+Dk+Zg54Pj\n/jVQLRgGgFTTzgfHufHu/Rw7PgFMfhfwjXfvBzAENJCcApJq2jY6dvrF/5RjxyfYNjrWo4qk9hgA\nUk2Hnzs2p3ap3xkAUk3LWnz3b6t2qd8ZAFJNfiewFhovAks1+Z3AWmgMAGkO/E5gLSQdmQKKiKsi\nYiwiDkbE1ibb3x0RD0fE/oj4RkS8phPjSpLmr+0AiIgh4BbgamAtcG1ErJ3W7bvAr2XmJcBHgB3t\njitJak8n3gFcBhzMzCcy8wXgTmBjY4fM/EZm/k+1ugdY0YFxJUlt6EQALAcONaw/VbW18gfAV1pt\njIjNEbE3IvYePXq0A+VJkprp6kXgiPh1JgPgDa36ZOYOqimikZGR7FJpUi0379zPHfcfYiKToQiu\nvXwlH910Sa/LkualEwEwDqxsWF9RtU0REb8EfA64OjOf7cC4UlfdvHM/n9/z5On1iczT64aABlEn\npoAeAC6KiAsj4mzgGmBXY4eIWAXcDbwnM7/TgTGlrrvj/kNzapf6XdvvADLzRETcAIwCQ8CtmXkg\nIq6vtm8H/gJ4GfCZiAA4kZkj7Y4tddNENp+RbNUu9buOXAPIzN3A7mlt2xuW3w+8vxNjSZI6w78F\nJEmFMgAkqVAGgCQVygCQpEIZAJJUKANAqmnRWTGndqnfGQBSTSdONv+8f6t2qd8ZAJJUKANAkgpl\nAEhSoQwASSqUASBJhTIAJKlQBoAkFcoAkKRCGQCSVCgDQJIKZQBIUqEMAEkqlAEgSYUyACSpUAaA\nJBXKAJCkQhkAklQoA0CSCmUASFKhDABJKpQBIEmF6kgARMRVETEWEQcjYmuT7RERn662PxwRr+3E\nuJKk+Ws7ACJiCLgFuBpYC1wbEWundbsauKh6bAY+2+64kqT2dOIdwGXAwcx8IjNfAO4ENk7rsxG4\nPSftAZZExAUdGFuSNE+dCIDlwKGG9aeqtrn2ASAiNkfE3ojYe/To0Q6UJ0lqpu8uAmfmjswcycyR\npUuX9rocSVqwOhEA48DKhvUVVdtc+0iSuqgTAfAAcFFEXBgRZwPXALum9dkFvLf6NNDrgP/NzCMd\nGFuSNE+L2n2CzDwRETcAo8AQcGtmHoiI66vt24HdwAbgIPAT4PfaHVeS1J62AwAgM3cz+SLf2La9\nYTmBD3RiLElSZ/TdRWBJUncYAJJUKANAkgplAEhSoQwASSqUASBJhTIAJKlQBoAkFcoAkKRCGQCS\nVCgDQJIKZQBIUqEMAEkqlAEgSYUyACSpUAaAJBXKAJCkQhkAklQoA0CqqdV/Fv8TaVB57ko1nZxj\nu9TvDABJKpQBIEmFMgAkqVAGgCQVygCQpEIZAJJUKANAkgrVVgBExLkR8bWIeKz695wmfVZGxL9F\nxLcj4kBEfKidMaVeOefFi+fULvW7dt8BbAXuzcyLgHur9elOAH+cmWuB1wEfiIi1bY4rdd1fvv1i\nzoqpbWfFZLs0iNoNgI3AbdXybcCm6R0y80hmfqta/hHwKLC8zXGlnhialgDT16VB0m4AnJ+ZR6rl\n7wPnz9Q5IlYD64D72xxX6rpto2Mcn8gpbccnkm2jYz2qSGrPotk6RMQ9wCuabLqpcSUzMyKySb9T\nz/NzwF3AH2bmD2fotxnYDLBq1arZypO65vBzx+bULvW7WQMgM69otS0ino6ICzLzSERcADzTot9i\nJl/8v5CZd88y3g5gB8DIyEjLQJG6bdmSYcabvNgvWzLcg2qk9rU7BbQLuK5avg740vQOERHA3wGP\nZuZftzme1DNb1q9hePHQlLbhxUNsWb+mRxVJ7Wk3AD4GXBkRjwFXVOtExLKI2F31eT3wHuBNEfFQ\n9djQ5rhS121at5x3XrqcoZi88DsUwTsvXc6mdX6mQYNp1imgmWTms8Cbm7QfBjZUy/8B+FEJDbyd\nD45z175xJnJyZnIik7v2jTPyynMNAQ0k7wSWato2Osax4xNT2o4dn/BTQBpYBoBUk58C0kJjAEg1\n/fxw8z/50Kpd6ncGgFRTtLiS1apd6ncGgFTTcz85Pqd2qd8ZAFJNrW748kYwDSoDQKrJG8G00LR1\nH4BUklOf9d82Osbh546xbMkwW9av8R4ADSwDQJqDTeu881cLh1NAklQoA0CSCmUASFKhDABJKpQB\nIEmFMgAkqVAGgCQVygCQpEIZAJJUKANAkgplAEhSoQwASSqUASBJhTIAJKlQBoAkFcoAkKRCGQCS\nVCgDQJIKZQBIUqHaCoCIODcivhYRj1X/njND36GIeDAivtzOmJKkzmj3HcBW4N7MvAi4t1pv5UPA\no22OJ0nqkHYDYCNwW7V8G7CpWaeIWAG8Ffhcm+NJkjqk3QA4PzOPVMvfB85v0e+TwJ8CJ9scT5LU\nIYtm6xAR9wCvaLLppsaVzMyIyCb7vw14JjP3RcQba4y3GdgMsGrVqtm6S5LmadYAyMwrWm2LiKcj\n4oLMPBIRFwDPNOn2euA3ImID8DPASyPi85n5uy3G2wHsABgZGfl/gSJJ6ox2p4B2AddVy9cBX5re\nITNvzMwVmbkauAb411Yv/pKk7mk3AD4GXBkRjwFXVOtExLKI2N1ucZKkM2fWKaCZZOazwJubtB8G\nNjRpvw+4r50xJUmd4Z3AklQoA0CSCtXWFJBUmpt37ueO+w8xkclQBNdevpKPbrqk12VJ82IASDXd\nvHM/n9/z5On1iczT64aABpFTQFJNd9x/aE7tUr8zAKSaJrL5fYmt2qV+ZwBINQ1FzKld6ncGgFTT\ntZevnFO71O+8CCzVdOpCr58C0kIR2cfzlyMjI7l3795elyFJAyMi9mXmSJ2+TgFJUqEMAEkqlAEg\nSYUyACSpUAaAJBXKAJCkQhkAklQoA0CSCtXXN4JFxFHge72uYxbnAT/odRE1DEqdMDi1WmfnDUqt\n/VznKzNzaZ2OfR0AgyAi9ta9666XBqVOGJxarbPzBqXWQalzNk4BSVKhDABJKpQB0L4dvS6gpkGp\nEwanVuvsvEGpdVDqnJHXACSpUL4DkKRCGQCViDg3Ir4WEY9V/57Tot9VETEWEQcjYmtD+29HxIGI\nOBkRI9P2ubHqPxYR6xvaL42I/dW2T0fM/t2CHaiz6f4R8e6IeKjhcTIifrnadl/1XKe2vXy2Os9w\nrasj4lhDPdv79JheGRH7qnr2RcSbGvaZ0zFtNXbD9qh+3oMR8XBEvHa+dVfbmp6zNY7lmahzW0T8\nd9X/ixGxpGpveR70qM4PR8R4Qz0bGrbN63iecZnpY3Ia7BPA1mp5K/DxJn2GgMeBVwFnA/8FrK22\n/QKwBrgPGGnYZ23V70XAhdX+Q9W2bwKvAwL4CnB1F+qss/8lwOMN61N+pi4e06b7A6uBR1qM2TfH\nFFgHLKuWfxEYn88xnWnshj4bqp83qp///jbqbnnO9qjOtwCLquWP1zkPelTnh4E/aTLevI5nNx6+\nA/ipjcBt1fJtwKYmfS4DDmbmE5n5AnBntR+Z+WhmjrV43jsz8/nM/C5wELgsIi4AXpqZe3LyLLm9\nxZgdrbPm/tdW+7SrG7We1m/HNDMfzMzDVfsBYDgiXlSjnrmM3fgz3J6T9gBLquMxn+Pb9JztVZ2Z\n+dXMPFHtvwdYUaOWrtc5g/kezzPOAPip8zPzSLX8feD8Jn2WA4ca1p+q2mbSap/l1fJcnqsTddbZ\n/3eAO6a13Va9rf3zOtMqXaj1wqqer0fErzY8V78e03cC38rM5xva6h7TOufdTOfZXOuez3l+Juts\n9PtM/mZ+SrPzoJd1frCaMrq1YUptvsfzjCvqS+Ej4h7gFU023dS4kpkZET37eFS36my2f0RcDvwk\nMx9paH53Zo5HxEuAu4D3MPnbda9qPQKsysxnI+JSYGdEXDzT/j0+phczOXXxlobmlse0F3p9ztcR\nETcBJ4AvVE1Nz4PM/GGPSvws8BEgq3//isnA6ltFBUBmXtFqW0Q8HREXZOaR6q3eM026jQMrG9ZX\nVG0zabXPOFPfyp5+rjNc52z7X8O03/4z81RdP4qIf2Ty7evtvaq1+i36+Wp5X0Q8DryaPjymEbEC\n+CLw3sx8/FT7TMd0jmPP1mfxPOqez3l+JuskIt4HvA14czW9N9N5sLcXdWbm0w31/i3w5TmM1xud\nvqgwqA9gG1MviH2iSZ9FwBNMXsg5dQHo4ml97mPqReCLmXoB6AlaXwTecKbrnGl/JqcEx4FXTXuu\n86rlxcA/A9d345i22h9Y2nAMX1XVfG6/HVNgSdXvN5s8V+1jWvO8eytTL1p+s426W56zsxzHM1Xn\nVcC3gaXTnqvledCjOi9o2P+PmJz3n/fx7Maj5wX0ywN4GXAv8BhwDz99QVkG7G7otwH4DpNX8m9q\naH8Hk3N7zwNPA6MN226q+o/R8KkUYAR4pNr2N1Q35p3hOpvuX217I7Bn2ng/C+wDHmbyQuan6p68\nZ6pWJufTDwAPAd8C3t6PxxS4GfhxVeepx8vnc0ybjQ1cTxUcTL5Q3VJt38/UX0Lmcy40PWdrHMsz\nUedBJufQTx3D7bOdBz2q8x+qvg8Du5gaCPM6nmf64Z3AklQoPwUkSYUyACSpUAaAJBXKAJCkQhkA\nklQoA0CSCmUASFKhDABJKtT/ASbK8oXCSY5qAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20bcf198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=config, graph=graph) as session:\n",
    "    saver.restore(session, \"/tmp/tensorflowmodels/LeNet_CNN.ckpt\")\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : images1[0:256], tf_train_labelset : stock_return[0:256], keep_prob : 1, training: True}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "\n",
    "plt.scatter(predictions,stock_return_f[0:256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2dffae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(predictions[0:140]-0.0008615,stock_return_f[0:140])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN based on LeNet-5 - 30 min middle of the day return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 256]\n"
     ]
    }
   ],
   "source": [
    "height = width\n",
    "channels = 1\n",
    "num_labels = 1\n",
    "\n",
    "#valid_batch_size = 5000\n",
    "#test_batch_size = 6508\n",
    "batch_size = 256\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 =64\n",
    "num_hidden = 256\n",
    "lamb_reg = 0.001\n",
    "learning_rate = 0.01  #  learning rate for the momentum optimizer\n",
    "\n",
    "reset_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, \n",
    "                                      shape=(batch_size, height, width, channels), name=\"TrainingData\")\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, \n",
    "                                     shape=(batch_size, num_labels), name=\"TrainingLabels\")\n",
    "    #tf_valid_dataset = tf.placeholder(tf.float32,\n",
    "    #                                 shape=(valid_batch_size, height, width, channels), name=\"ValidationData\")\n",
    "    #tf_test_dataset = tf.placeholder(tf.float32,\n",
    "     #                                shape=(test_batch_size, height, width, channels), name=\"TestingData\")\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "    # Variables \n",
    "    layer1_weights = weight_variable([patch_size, patch_size, channels, depth1], name=\"weights1\")\n",
    "    layer1_biases = bias_variable([depth1], name=\"biases1\")\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth1, depth2], name=\"weights2\")\n",
    "    layer2_biases = bias_variable([depth2], name=\"biases2\")    \n",
    "    layer3_weights = weight_variable([height // 4 * width // 4 * depth2, num_hidden], name=\"weights3\")\n",
    "    layer3_biases = bias_variable([num_hidden], name=\"biases3\")    # Model with dropout\n",
    "    layer4_weights = weight_variable([num_hidden, num_labels], name=\"weights4\")\n",
    "    layer4_biases = bias_variable([num_labels], name=\"biases4\")\n",
    "            \n",
    "    def model(data, name, proba=keep_prob):\n",
    "            # Model.\n",
    "            inputs = tf.cast(normalize_simple(data),dtype=tf.float32)\n",
    "            #inputs = tf.cast(data,dtype=tf.float32)\n",
    "            #inputs = LeCunLCN(data, data.get_shape().as_list())\n",
    "            # Convolution\n",
    "            conv_1 = tf.nn.bias_add(tf.nn.conv2d(inputs, layer1_weights, [1, 1, 1, 1], padding='SAME'), layer1_biases, name=\"layer1\")\n",
    "            pool_1 = tf.nn.max_pool(tf.nn.relu(conv_1), ksize=[1, 2, 2, 1],\n",
    "                                     strides=[1, 2, 2, 1], padding='SAME', name=\"pooled1\")\n",
    "            # Convolution\n",
    "            conv_2 = tf.nn.bias_add(tf.nn.conv2d(pool_1, layer2_weights, [1, 1, 1, 1], padding='SAME'), layer2_biases, name=\"layer2\")\n",
    "            pool_2 = tf.nn.max_pool(tf.nn.relu(conv_2), ksize=[1, 2, 2, 1],\n",
    "                                     strides=[1, 2, 2, 1], padding='SAME', name=\"pooled2\")\n",
    "            # Fully Connected Layer\n",
    "            shape = pool_2.get_shape().as_list()\n",
    "            #print (shape)\n",
    "            reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            full_3 = tf.nn.bias_add(tf.matmul(reshape, layer3_weights),layer3_biases)\n",
    "            \n",
    "            # Dropout\n",
    "            full_4 = tf.nn.dropout(full_3, proba)\n",
    "            print(full_4.get_shape().as_list())\n",
    "            layer_fc = tf.nn.bias_add(tf.matmul(full_4, layer4_weights),layer4_biases, name=\"layer_fc\")\n",
    "            return layer_fc\n",
    "  \n",
    "   # Training computation.\n",
    "    logits = model(tf_train_dataset, \"logits\", keep_prob)\n",
    "    loss=tf.nn.l2_loss(logits-tf_train_labelset, name=\"loss\")\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels = tf_train_labelset), name=\"loss\")\n",
    "    #regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "     #               tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "     #               tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "     #               tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases) )\n",
    "\n",
    "    # Add the regularization term to the loss.\n",
    "    #loss += lamb_reg * regularizers\n",
    "    #loss = tf.reduce_mean(loss + lamb_reg * regularizers)\n",
    "    \n",
    "    # Optimizer\n",
    "    #global_step = tf.Variable(0, name=\"globalstep\")  # count  number of steps taken.\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9).minimize(loss, global_step=global_step)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = logits  #tf.nn.softmax(logits)\n",
    "    #valid_prediction = tf.nn.softmax(model(tf_valid_dataset, \"validation\", 1.0))  # no dropout\n",
    "    #test_prediction = tf.nn.softmax(model(tf_test_dataset, \"testing\", 1.0))  # no dropout\n",
    "    saver = tf.train.Saver()   # a saver variable to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Session function derived from that of last Section\n",
    "# Specify the number of epochs, name to give the model and the dropout probability\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "def run_session(num_epochs, name, k_prob=1.0):\n",
    "    \n",
    "    with tf.Session(config=config, graph=graph) as session:\n",
    "            tf.global_variables_initializer().run() \n",
    "            loss_summary = tf.summary.scalar('Loss', loss)\n",
    "            learning_rate_summary = tf.summary.scalar('Learning_rate', learning_rate)\n",
    "            merged = tf.summary.merge_all()  \n",
    "            writer = tf.summary.FileWriter(\"/tmp/tensorflowlogs\", session.graph)\n",
    "            print(\"Initialized\\n\")\n",
    "            epoch = 0\n",
    "            for step in range(int(stock_return_30min_f.shape[0]/batch_size)*num_epochs):\n",
    "                #epoch = int(20*train_labels.shape[0]/batch_size)\n",
    "                offset = (step * batch_size) % (stock_return_f.shape[0] - batch_size)\n",
    "                batch_data = images_f[offset:(offset + batch_size)]\n",
    "                batch_labels = stock_return_30min_f[offset:(offset + batch_size)]\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labelset : batch_labels, keep_prob : k_prob, training: True}\n",
    "                _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "                writer.add_summary(loss_summary.eval(feed_dict=feed_dict), epoch)\n",
    "                writer.add_summary(learning_rate_summary.eval(), epoch)\n",
    "                if (offset < batch_size):\n",
    "                    epoch += 1\n",
    "                    if (epoch % 1) == 0:\n",
    "                        print(\"Minibatch loss at epoch {}: {}\".format(epoch, l))\n",
    "                        #print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "                        true_preds = 0\n",
    "                        #for batch_num in range(int(valid_labels.shape[0]/valid_batch_size)): \n",
    "                            #batch_valid_data = valid_data[batch_num*valid_batch_size:(batch_num+1)*valid_batch_size]\n",
    "                            #batch_valid_labels = valid_labels[batch_num*valid_batch_size:(batch_num+1)*valid_batch_size]\n",
    "                         #   feed_dict = {tf_valid_dataset : batch_valid_data}\n",
    "                         #   predictions = session.run(valid_prediction, feed_dict) \n",
    "                          #  true_preds += np.sum(np.argmax(predictions, 1) == np.argmax(batch_valid_labels, 1))\n",
    "                        #print(\"Validation accuracy: {:.1f}\\n\".format(100.0 * true_preds / valid_data.shape[0]))\n",
    "\n",
    "            save_path = saver.save(session, \"/tmp/tensorflowmodels/\" + name +\".ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Minibatch loss at epoch 1: 0.46967238187789917\n",
      "Minibatch loss at epoch 2: 1.2189757823944092\n",
      "Minibatch loss at epoch 3: 2.3922605514526367\n",
      "Minibatch loss at epoch 4: 0.5110006928443909\n",
      "Minibatch loss at epoch 5: 2.0223395824432373\n",
      "Minibatch loss at epoch 6: 2.810458183288574\n",
      "Minibatch loss at epoch 7: 0.5695533752441406\n",
      "Minibatch loss at epoch 8: 1.9983874559402466\n",
      "Minibatch loss at epoch 9: 2.9185116291046143\n",
      "Minibatch loss at epoch 10: 0.5768954157829285\n",
      "Minibatch loss at epoch 11: 1.952029824256897\n",
      "Minibatch loss at epoch 12: 3.0403335094451904\n",
      "Minibatch loss at epoch 13: 0.5778136849403381\n",
      "Minibatch loss at epoch 14: 1.9996447563171387\n",
      "Minibatch loss at epoch 15: 3.1895031929016113\n",
      "Minibatch loss at epoch 16: 0.5825947523117065\n",
      "Minibatch loss at epoch 17: 1.99739670753479\n",
      "Minibatch loss at epoch 18: 3.2712578773498535\n",
      "Minibatch loss at epoch 19: 0.7186753749847412\n",
      "Minibatch loss at epoch 20: 2.4173812866210938\n",
      "Minibatch loss at epoch 21: 0.4754016697406769\n",
      "Minibatch loss at epoch 22: 1.6200473308563232\n",
      "Minibatch loss at epoch 23: 2.5471062660217285\n",
      "Minibatch loss at epoch 24: 0.5693472027778625\n",
      "Minibatch loss at epoch 25: 2.0235345363616943\n",
      "Minibatch loss at epoch 26: 2.903571844100952\n",
      "Minibatch loss at epoch 27: 0.574628472328186\n",
      "Minibatch loss at epoch 28: 2.0044898986816406\n",
      "Minibatch loss at epoch 29: 2.9771728515625\n",
      "Minibatch loss at epoch 30: 0.5799347162246704\n",
      "Minibatch loss at epoch 31: 1.9191522598266602\n",
      "Minibatch loss at epoch 32: 3.168384313583374\n",
      "Minibatch loss at epoch 33: 0.5803642272949219\n",
      "Minibatch loss at epoch 34: 2.0341873168945312\n",
      "Minibatch loss at epoch 35: 3.237414836883545\n",
      "Minibatch loss at epoch 36: 0.678124725818634\n",
      "Minibatch loss at epoch 37: 2.4167237281799316\n",
      "Minibatch loss at epoch 38: 0.47278138995170593\n",
      "Minibatch loss at epoch 39: 1.2345409393310547\n",
      "Minibatch loss at epoch 40: 2.4519104957580566\n",
      "Minibatch loss at epoch 41: 0.5381476283073425\n",
      "Minibatch loss at epoch 42: 2.030104637145996\n",
      "Minibatch loss at epoch 43: 2.8205771446228027\n",
      "Minibatch loss at epoch 44: 0.5748809576034546\n",
      "Minibatch loss at epoch 45: 2.0006649494171143\n",
      "Minibatch loss at epoch 46: 2.9653871059417725\n",
      "Minibatch loss at epoch 47: 0.5785481929779053\n",
      "Minibatch loss at epoch 48: 1.9475524425506592\n",
      "Minibatch loss at epoch 49: 3.049867630004883\n",
      "Minibatch loss at epoch 50: 0.5819748640060425\n",
      "Minibatch loss at epoch 51: 1.9969592094421387\n",
      "Minibatch loss at epoch 52: 3.1960177421569824\n",
      "Minibatch loss at epoch 53: 0.5869556069374084\n",
      "Minibatch loss at epoch 54: 2.1062607765197754\n",
      "Minibatch loss at epoch 55: 3.2698464393615723\n",
      "Minibatch loss at epoch 56: 0.9874272346496582\n",
      "Minibatch loss at epoch 57: 2.4159250259399414\n",
      "Minibatch loss at epoch 58: 0.47722697257995605\n",
      "Minibatch loss at epoch 59: 1.6459269523620605\n",
      "Minibatch loss at epoch 60: 2.704148292541504\n",
      "Minibatch loss at epoch 61: 0.5710346102714539\n",
      "Minibatch loss at epoch 62: 2.0210399627685547\n",
      "Minibatch loss at epoch 63: 2.918269634246826\n",
      "Minibatch loss at epoch 64: 0.5757559537887573\n",
      "Minibatch loss at epoch 65: 1.9695401191711426\n",
      "Minibatch loss at epoch 66: 2.982740879058838\n",
      "Minibatch loss at epoch 67: 0.5789728164672852\n",
      "Minibatch loss at epoch 68: 1.9780166149139404\n",
      "Minibatch loss at epoch 69: 3.1720447540283203\n",
      "Minibatch loss at epoch 70: 0.5800647139549255\n",
      "Minibatch loss at epoch 71: 2.0029337406158447\n",
      "Minibatch loss at epoch 72: 3.237423896789551\n",
      "Minibatch loss at epoch 73: 0.7204300165176392\n",
      "Minibatch loss at epoch 74: 2.4205515384674072\n",
      "Minibatch loss at epoch 75: 0.4735950231552124\n",
      "Minibatch loss at epoch 76: 1.548856496810913\n",
      "Minibatch loss at epoch 77: 2.467677116394043\n",
      "Minibatch loss at epoch 78: 0.539618968963623\n",
      "Minibatch loss at epoch 79: 2.022841453552246\n",
      "Minibatch loss at epoch 80: 2.8257784843444824\n",
      "Minibatch loss at epoch 81: 0.5780348181724548\n",
      "Minibatch loss at epoch 82: 1.9978994131088257\n",
      "Minibatch loss at epoch 83: 2.9652416706085205\n",
      "Minibatch loss at epoch 84: 0.574171781539917\n",
      "Minibatch loss at epoch 85: 1.9542477130889893\n",
      "Minibatch loss at epoch 86: 3.0548276901245117\n",
      "Minibatch loss at epoch 87: 0.5805449485778809\n",
      "Minibatch loss at epoch 88: 1.9789764881134033\n",
      "Minibatch loss at epoch 89: 3.2038068771362305\n",
      "Minibatch loss at epoch 90: 0.5927343964576721\n",
      "Minibatch loss at epoch 91: 2.1087920665740967\n",
      "Minibatch loss at epoch 92: 3.271158218383789\n",
      "Minibatch loss at epoch 93: 1.0053565502166748\n",
      "Minibatch loss at epoch 94: 2.4249305725097656\n",
      "Minibatch loss at epoch 95: 0.4895905554294586\n",
      "Minibatch loss at epoch 96: 1.9295620918273926\n",
      "Minibatch loss at epoch 97: 2.7092552185058594\n",
      "Minibatch loss at epoch 98: 0.5728661417961121\n",
      "Minibatch loss at epoch 99: 2.0196380615234375\n",
      "Minibatch loss at epoch 100: 2.9186553955078125\n",
      "Minibatch loss at epoch 101: 0.5747005343437195\n",
      "Minibatch loss at epoch 102: 1.9658745527267456\n",
      "Model saved in file: /tmp/tensorflowmodels/LeNet_CNN_30min.ckpt\n"
     ]
    }
   ],
   "source": [
    "run_session(100, \"LeNet_CNN_30min\", 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tensorflowmodels/LeNet_CNN_30min.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20bcfe48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=config, graph=graph) as session:\n",
    "    saver.restore(session, \"/tmp/tensorflowmodels/LeNet_CNN_30min.ckpt\")\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : images1[0:256], tf_train_labelset : stock_return[0:256], keep_prob : 1, training: True}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "\n",
    "plt.scatter(predictions,stock_return_f[0:256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet Deep Learning architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256, 30, 30, 64]\n",
      "[256, 32, 32, 64]\n",
      "[256, 14, 14, 128]\n",
      "[256, 30, 30, 128]\n",
      "[256, 3, 3, 256]\n",
      "[256, 7, 7, 256]\n",
      "[256, 3, 3, 256]\n",
      "[256, 1024]\n",
      "[256, 1024]\n",
      "[256, 1]\n",
      "[256, 1]\n"
     ]
    }
   ],
   "source": [
    "width=96\n",
    "height = width\n",
    "channels = 1\n",
    "num_labels = 1\n",
    "\n",
    "#valid_batch_size = 5000\n",
    "#test_batch_size = 6508\n",
    "batch_size = 256\n",
    "\n",
    "## ///// Model definition of parameters\n",
    "conv1_fmaps = 64\n",
    "conv1_ksize = [11,11]\n",
    "conv1_stride = 3\n",
    "conv1_pad = \"SAME\"\n",
    "pool1_ksize = [3,3]\n",
    "pool1_stride = 1\n",
    "\n",
    "conv2_fmaps = 128\n",
    "conv2_ksize = [7,7]\n",
    "conv2_stride = 1\n",
    "conv2_pad = \"SAME\"\n",
    "pool2_ksize = [3,3]\n",
    "pool2_stride = 2\n",
    "\n",
    "conv3_fmaps = 256\n",
    "conv3_ksize = [7,7]\n",
    "conv3_stride = 2\n",
    "conv3_pad = \"SAME\"\n",
    "pool3_ksize = [3,3]\n",
    "pool3_stride = 2\n",
    "\n",
    "num_fcl1 = 1024\n",
    "num_fcl2 = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, \n",
    "                                      shape=(batch_size, height, width, channels), name=\"TrainingData\")\n",
    "    tf_train_labelset = tf.placeholder(tf.float32, \n",
    "                                     shape=(batch_size, num_labels), name=\"TrainingLabels\")\n",
    "    #tf_valid_dataset = tf.placeholder(tf.float32,\n",
    "    #                                 shape=(valid_batch_size, height, width, channels), name=\"ValidationData\")\n",
    "    #tf_test_dataset = tf.placeholder(tf.float32,\n",
    "     #                                shape=(test_batch_size, height, width, channels), name=\"TestingData\")\n",
    "    \n",
    "    drop_prob = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "    training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "    # Model with dropout\n",
    "    def model(data):\n",
    "        inputs = tf.cast(normalize_simple(data),dtype=tf.float32)\n",
    "        #inputs = tf.cast(data,dtype=tf.float32)\n",
    "        #inputs = LeCunLCN(data, data.get_shape().as_list())\n",
    "        with tf.variable_scope(\"layer1\") as scope:\n",
    "            # Convolution\n",
    "            conv_1 = tf.layers.conv2d(inputs=inputs, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                                      strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu)\n",
    "            # Max pooling\n",
    "            pool_1 = tf.layers.max_pooling2d(inputs=conv_1, pool_size=pool1_ksize, strides=pool1_stride)\n",
    "            # Normalization and dropout\n",
    "            norm_1 = tf.nn.lrn(pool_1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "            drop_1 = tf.layers.dropout(norm_1, rate=drop_prob, training=training)\n",
    "\n",
    "        with tf.variable_scope(\"layer2\") as scope:\n",
    "            conv_2 = tf.layers.conv2d(inputs=drop_1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                                      strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu)\n",
    "            pool_2 = tf.layers.max_pooling2d(inputs=conv_2, pool_size=pool2_ksize, strides=pool2_stride)\n",
    "            norm_2 = tf.nn.lrn(pool_2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "            drop_2 = tf.layers.dropout(norm_2, rate=drop_prob, training=training)\n",
    "\n",
    "        with tf.variable_scope(\"layer3\") as scope:\n",
    "            conv_3 = tf.layers.conv2d(inputs=drop_2, filters=conv3_fmaps, kernel_size=conv3_ksize,\n",
    "                                      strides=conv3_stride, padding=conv3_pad, activation=tf.nn.relu)\n",
    "            pool_3 = tf.layers.max_pooling2d(inputs=conv_3, pool_size=pool3_ksize, strides=pool3_stride)\n",
    "            norm_3 = tf.nn.lrn(pool_3, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "            drop_3 = tf.layers.dropout(norm_3, rate=drop_prob, training=training)\n",
    "\n",
    "        with tf.variable_scope(\"layer4\") as scope:\n",
    "            print(pool_1.get_shape().as_list())\n",
    "            print(conv_1.get_shape().as_list())\n",
    "            print(pool_2.get_shape().as_list())\n",
    "            print(conv_2.get_shape().as_list())\n",
    "            print(pool_3.get_shape().as_list())\n",
    "            print(conv_3.get_shape().as_list())\n",
    "            reshape_flat = tf.reshape(drop_3, [-1, 3*3*conv3_fmaps])\n",
    "            full_4 = tf.layers.dense(inputs=reshape_flat, units=num_fcl1, activation=tf.nn.tanh)\n",
    "            print(drop_3.get_shape().as_list())\n",
    "            print(full_4.get_shape().as_list())\n",
    "\n",
    "        with tf.variable_scope(\"layer5\") as scope:\n",
    "            full_5 = tf.layers.dense(inputs=full_4, units=num_fcl2, activation=tf.nn.tanh)\n",
    "\n",
    "        with tf.variable_scope(\"layer6\") as scope:\n",
    "            full_6 = tf.layers.dense(inputs=full_5, units=num_labels, activation=tf.nn.tanh)\n",
    "            print(full_5.get_shape().as_list())\n",
    "        return full_6\n",
    "    \n",
    "    # Training computation.\n",
    "    with tf.variable_scope(\"model\"):   \n",
    "        logits = model(tf_train_dataset)\n",
    "        print(logits.get_shape().as_list())\n",
    "        print(tf_train_labelset.get_shape().as_list())\n",
    "        loss = tf.nn.l2_loss(logits-tf_train_labelset, name=\"loss\")\n",
    "        \n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits, name=\"training\")\n",
    "    #with tf.variable_scope(\"model\", reuse=True):   \n",
    "        #valid_prediction = tf.nn.softmax(model(tf_valid_dataset), name=\"Validation\")  # no dropout\n",
    "    #with tf.variable_scope(\"model\", reuse=True):   \n",
    "    #    test_prediction = tf.nn.softmax(model(tf_test_dataset), name=\"Testing\")  # no dropout\n",
    "    saver = tf.train.Saver()   # a saver variable to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "def run_session(num_epochs, name, d_prob=0.0):\n",
    "    \n",
    "    with tf.Session(config=config, graph=graph) as session:\n",
    "            tf.global_variables_initializer().run() \n",
    "            loss_summary = tf.summary.scalar('Loss', loss)\n",
    "            learning_rate_summary = tf.summary.scalar('Learning_rate', learning_rate)\n",
    "            merged = tf.summary.merge_all()  \n",
    "            writer = tf.summary.FileWriter(\"/tmp/tensorflowlogs\", session.graph)\n",
    "            print(\"Initialized\\n\")\n",
    "            epoch = 0\n",
    "            for step in range(int(stock_return_30min_f.shape[0]/batch_size)*num_epochs):\n",
    "                #epoch = int(20*train_labels.shape[0]/batch_size)\n",
    "                offset = (step * batch_size) % (stock_return_30min_f.shape[0] - batch_size)\n",
    "                batch_data = images_f[offset:(offset + batch_size)]\n",
    "                batch_labels = stock_return_30min_f[offset:(offset + batch_size)]\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labelset : batch_labels, drop_prob : d_prob, training: True}\n",
    "                _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "                writer.add_summary(loss_summary.eval(feed_dict=feed_dict), epoch)\n",
    "                writer.add_summary(learning_rate_summary.eval(), epoch)\n",
    "                if (offset < batch_size):\n",
    "                    epoch += 1\n",
    "                    if (epoch % 1) == 0:\n",
    "                        print(\"Minibatch loss at epoch {}: {}\".format(epoch, l))\n",
    "                        #print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "                        true_preds = 0\n",
    "                        #for batch_num in range(int(valid_labels.shape[0]/valid_batch_size)): \n",
    "                        #    batch_valid_data = valid_data[batch_num*valid_batch_size:(batch_num+1)*valid_batch_size]\n",
    "                        #    batch_valid_labels = valid_labels[batch_num*valid_batch_size:(batch_num+1)*valid_batch_size]\n",
    "                        #    feed_dict = {tf_valid_dataset : batch_valid_data, drop_prob : d_prob}\n",
    "                        #    predictions = session.run(valid_prediction, feed_dict) \n",
    "                        #    true_preds += np.sum(np.argmax(predictions, 1) == np.argmax(batch_valid_labels, 1))\n",
    "                        #print(\"Validation accuracy: {:.1f}\\n\".format(100.0 * true_preds / valid_data.shape[0]))\n",
    "                        #for batch_num in range(int(test_data.shape[0]/test_batch_size)): \n",
    "                        #    batch_test_data = test_data[batch_num*test_batch_size:(batch_num+1)*test_batch_size]\n",
    "                        #    feed_dict = {tf_test_dataset : batch_test_data, drop_prob : 0.5}\n",
    "                        #    predictions_test = session.run(test_prediction, feed_dict)\n",
    "\n",
    "            save_path = saver.save(session, \"/tmp/tensorflowmodels/\" + name +\".ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Minibatch loss at epoch 1: 32.57305908203125\n",
      "Minibatch loss at epoch 2: 2.6359269618988037\n",
      "Minibatch loss at epoch 3: 2.445629596710205\n",
      "Minibatch loss at epoch 4: 0.513069748878479\n",
      "Minibatch loss at epoch 5: 2.038557529449463\n",
      "Minibatch loss at epoch 6: 2.808361291885376\n",
      "Minibatch loss at epoch 7: 0.5872829556465149\n",
      "Minibatch loss at epoch 8: 2.015974521636963\n",
      "Minibatch loss at epoch 9: 2.9292848110198975\n",
      "Minibatch loss at epoch 10: 0.5967376232147217\n",
      "Minibatch loss at epoch 11: 1.9864485263824463\n",
      "Minibatch loss at epoch 12: 3.039201021194458\n",
      "Minibatch loss at epoch 13: 0.5880846977233887\n",
      "Minibatch loss at epoch 14: 1.996126413345337\n",
      "Minibatch loss at epoch 15: 3.218116283416748\n",
      "Minibatch loss at epoch 16: 0.5967973470687866\n",
      "Minibatch loss at epoch 17: 2.00003981590271\n",
      "Minibatch loss at epoch 18: 3.307478904724121\n",
      "Minibatch loss at epoch 19: 0.7421005964279175\n",
      "Minibatch loss at epoch 20: 2.4270899295806885\n",
      "Minibatch loss at epoch 21: 0.4683454930782318\n",
      "Minibatch loss at epoch 22: 1.6293623447418213\n",
      "Minibatch loss at epoch 23: 2.5654454231262207\n",
      "Minibatch loss at epoch 24: 0.5942153930664062\n",
      "Minibatch loss at epoch 25: 2.0474820137023926\n",
      "Minibatch loss at epoch 26: 2.8961899280548096\n",
      "Minibatch loss at epoch 27: 0.5791596174240112\n",
      "Minibatch loss at epoch 28: 2.012767791748047\n",
      "Minibatch loss at epoch 29: 2.9899282455444336\n",
      "Minibatch loss at epoch 30: 0.579322874546051\n",
      "Minibatch loss at epoch 31: 1.9241931438446045\n",
      "Minibatch loss at epoch 32: 3.1610560417175293\n",
      "Minibatch loss at epoch 33: 0.5892672538757324\n",
      "Minibatch loss at epoch 34: 2.0421321392059326\n",
      "Minibatch loss at epoch 35: 3.2403664588928223\n",
      "Minibatch loss at epoch 36: 0.681020975112915\n",
      "Minibatch loss at epoch 37: 2.4268033504486084\n",
      "Minibatch loss at epoch 38: 0.4728466272354126\n",
      "Minibatch loss at epoch 39: 1.2342404127120972\n",
      "Minibatch loss at epoch 40: 2.4630484580993652\n",
      "Minibatch loss at epoch 41: 0.5410010814666748\n",
      "Minibatch loss at epoch 42: 2.0173089504241943\n",
      "Minibatch loss at epoch 43: 2.813647747039795\n",
      "Minibatch loss at epoch 44: 0.5760962963104248\n",
      "Minibatch loss at epoch 45: 1.9911882877349854\n",
      "Minibatch loss at epoch 46: 2.957437038421631\n",
      "Minibatch loss at epoch 47: 0.5798496007919312\n",
      "Minibatch loss at epoch 48: 1.9544172286987305\n",
      "Minibatch loss at epoch 49: 3.062293767929077\n",
      "Minibatch loss at epoch 50: 0.5840722918510437\n",
      "Minibatch loss at epoch 51: 2.0360000133514404\n",
      "Model saved in file: /tmp/tensorflowmodels/AlexNet_NN_30min.ckpt\n"
     ]
    }
   ],
   "source": [
    "run_session(50, \"AlexNet_NN_30min\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tensorflowmodels/AlexNet_NN_30min.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGQCSVCgDQJIKZQBIUqH+D6k8Ex+pK8SrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eaf6400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=config, graph=graph) as session:\n",
    "    saver.restore(session, \"/tmp/tensorflowmodels/AlexNet_NN_30min.ckpt\")\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : images1[0:256], tf_train_labelset : stock_return[0:256], drop_prob : 0, training: True}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "\n",
    "plt.scatter(predictions,stock_return_f[0:256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
